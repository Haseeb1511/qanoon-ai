{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baa77a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "# from langchain_postgres import PGVector\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,add_messages,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import RemoveMessage # to delete something from state permenantly\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=r\"C:\\Users\\hasee\\Desktop\\Legal Chatbot\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdde66b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CONNECTION_STRING = os.environ.get(\"CONNECTION_STRING\",\"\")\n",
    "SUPERBASE_SERVICE_ROLE_KEY = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\",\"\")\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e41acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL,SUPERBASE_SERVICE_ROLE_KEY)\n",
    "print(\"Succefully coonectd to Supabase client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a848c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chatting llm\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                streaming=True,\n",
    "                stream_usage=True,\n",
    "                )\n",
    "                \n",
    "#embedding llm\n",
    "EMBEDDING = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97f548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated,Dict,Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict,total=False):\n",
    "    documents_path:str\n",
    "    documents:list[Document]\n",
    "    chunks:list[Document] \n",
    "    collection_name:str\n",
    "    retrieved_docs:list[Document]\n",
    "    context: str \n",
    "    answer:str\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    doc_id:str\n",
    "    user_id:str   # for tenant isolation\n",
    "    summary:str\n",
    "    vectorstore_uploaded:bool\n",
    "    rewritten_query:str\n",
    "    # token_usage: int\n",
    "    token_usage: Dict[str, Any]\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "        You are an expert Legal AI Assistant for Pakistan. Your task is to answer legal questions based strictly on the provided context.\n",
    "\n",
    "        Instructions:\n",
    "        1. Source-Based Answering: Answer the question using ONLY the information provided in the Context below. Do not use outside knowledge.\n",
    "        2. Specific Legal Citations: When making a statement, you must cite the specific legal authority found in the text (e.g., \"Article 6 of the Constitution\", \"Section 302 of PPC\", \"Clause 3\"). \n",
    "        3. Citation Format: Format citations as: [Legal Reference]** (Found in: Chunk ID/Source).\n",
    "            Example: \"Every citizen has the right to a fair trial as per Article 10-A (Source: Chunk 2, constitution.pdf)..\"\n",
    "        4. No Hallucinations:** If the provided context does not contain the answer, state: \"The provided context does not contain sufficient information to answer this question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83750b",
   "metadata": {},
   "source": [
    "# Document ID generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def get_file_hash(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    It reads a file (like a PDF) and generates a SHA-256 hash,\n",
    "      which is a fixed-length unique string representing the file’s content.\n",
    "\n",
    "    If:\n",
    "    The file content is exactly the same → hash is the same\n",
    "    Even 1 byte changes → hash is completely different\n",
    "    \"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# get_file_hash() reads the entire file content and produces a SHA-256 hash.\n",
    "# SHA-256 guarantees:\n",
    "# Same content → same hash → same doc_id\n",
    "# Even 1 byte difference → completely different hash\n",
    "# So if a user uploads the same PDF file again, the hash will be identical → same doc_id.\n",
    "\n",
    "\n",
    "# Even one single byte change in the file will produce a completely different hash.\n",
    "# That means if you change one word in the PDF, the doc_id will be different, because the file content is no longer exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1448f4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def set_doc_id(self,state:AgentState):\n",
    "        # Skip if doc_id already exists \n",
    "        if state.get(\"doc_id\"):\n",
    "            return state\n",
    "            \n",
    "        path = os.path.abspath(state[\"documents_path\"])\n",
    "        \n",
    "        if not os.path.isfile(path):\n",
    "            raise ValueError(\"Directoy uploaded not supported with hashing yet\")\n",
    "        state[\"doc_id\"] = get_file_hash(path)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7beaf85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# as uspbase = network call  so we make async function to avoid blocking the main thread and also we can do other task while waiting for response from supbase\n",
    "    async def check_pdf_already_uploaded(self,state:AgentState):\n",
    "        \"\"\"Checkif PDF already exist in SUpbase\n",
    "        Same PDF:    \n",
    "        - Different user\n",
    "        - Will embed again (correct behavior)\n",
    "        \"\"\"\n",
    "        # first check if vectostore already exist\n",
    "        if state.get(\"vectorstore_uploaded\"):\n",
    "            return state\n",
    "        # we check id documnet exist already or not and also check for user  if for sepcific user it exist or not (sometime one user might have already uploaded the same document )\n",
    "        \n",
    "        # as supbase client does not suppor async we use run in threadpool\n",
    "        # run_in_threadpool expects a callable (lambda), not the result of the call\n",
    "        response = await run_in_threadpool(\n",
    "            lambda: self.supabase_client.table(\"documents\")\n",
    "                        .select(\"doc_id\")\n",
    "                        .eq(\"doc_id\", state[\"doc_id\"])\n",
    "                        .eq(\"user_id\", state[\"user_id\"])\n",
    "                        .limit(1)\n",
    "                        .execute()\n",
    "        )\n",
    "        if response.data:\n",
    "            print(\"Pdf already exist in supbase skipping documnet ingesion...\")\n",
    "            state[\"vectorstore_uploaded\"] = True\n",
    "        else:\n",
    "            state[\"vectorstore_uploaded\"] = False\n",
    "        return state  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32947e5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def document_ingestion(self,state: AgentState):\n",
    "\n",
    "        if state.get(\"vectorstore_uploaded\"):\n",
    "            print(\"Skipping vectoingestion - PDF already exist\")\n",
    "            state[\"vectorstore_uploaded\"] = True\n",
    "            return state\n",
    "        \n",
    "        path = os.path.abspath(state[\"documents_path\"])  # ensure absolute\n",
    "\n",
    "        if not os.path.isfile(path):\n",
    "            raise ValueError(f\"Invalid documents_path: {path}\")\n",
    "        \n",
    "\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        # langchain chunk metadata is first updated\n",
    "        # langchain chunk metadata (each chunk of document will have this metadata (it will not have page content - only metadata))\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            source_path = chunk.metadata.get(\"source\",\"\")\n",
    "            file_name = os.path.basename(source_path) if source_path else \"unknow.pdf\"\n",
    "\n",
    "            metadata = {\n",
    "                \"user_id\":state[\"user_id\"],\n",
    "                \"doc_id\":state[\"doc_id\"],\n",
    "                \"chunk_index\":i,\n",
    "                \"file_name\":file_name,\n",
    "                \"page\":chunk.metadata.get(\"page\")  \n",
    "            }\n",
    "            # update langchian chunk metadata usd by pgvector\n",
    "            chunk.metadata.update(metadata)\n",
    "\n",
    "        vectorstore = PGVector(\n",
    "            connection=CONNECTION_STRING,\n",
    "            collection_name=state[\"collection_name\"],\n",
    "            embeddings=self.embedding_model,\n",
    "            use_jsonb=True,\n",
    "            engine_args={\"poolclass\": NullPool}  # disable pooling\n",
    "        )\n",
    "        batch_size=50\n",
    "        # upload embedding \n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading chunks\"):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            # async \n",
    "            await run_in_threadpool(vectorstore.add_documents, batch)\n",
    "            # vectorstore.add_documents(batch)\n",
    "        \n",
    "\n",
    "        # Insert metadata to supbase table\n",
    "        rows = [{   \n",
    "                \"user_id\":state[\"user_id\"],\n",
    "                \"doc_id\": state[\"doc_id\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"file_name\": chunk.metadata[\"file_name\"],\n",
    "                \"page\": chunk.metadata.get(\"page\"),\n",
    "                \"content\": chunk.page_content,\n",
    "        } for i,chunk in enumerate(chunks)\n",
    "        ]\n",
    "        if rows:\n",
    "            try:\n",
    "                await run_in_threadpool(\n",
    "                    lambda: self.supabase_client.table(\"documents\").insert(rows).execute()\n",
    "                )\n",
    "            except Exception:\n",
    "                print(\"Chunks already exist — skipping insert\")\n",
    "    \n",
    "        print(f\"Uploaded {len(chunks)} chunks\")\n",
    "\n",
    "        state[\"vectorstore_uploaded\"] = True\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8e67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def query_rewriter(self,state: AgentState):\n",
    "        \"\"\"Rewrite follow-up questions to be standalone using conversation context\"\"\"\n",
    "        \n",
    "        human_messages = [m for m in state.get(\"messages\", []) if isinstance(m, HumanMessage)]\n",
    "        current_query = human_messages[-1].content\n",
    "        \n",
    "        # If there's conversation history, rewrite the query\n",
    "        if len(state.get(\"messages\", [])) > 1:\n",
    "            \n",
    "            # Build conversation context\n",
    "            memory_text = state.get(\"summary\") or \"\"\n",
    "            if not memory_text:\n",
    "                conversation_history = []\n",
    "                for m in state.get(\"messages\", [])[:-1]:  # Exclude current question\n",
    "                    role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
    "                    conversation_history.append(f\"{role}: {m.content}\")\n",
    "                memory_text = \"\\n\".join(conversation_history)\n",
    "            \n",
    "            # Rewrite query to be standalone\n",
    "            rewrite_prompt = f\"\"\"Given this conversation history:\n",
    "                {memory_text}\n",
    "\n",
    "                Rewrite the following question to be standalone (include necessary context from history):\n",
    "                Question: {current_query}\n",
    "\n",
    "                Standalone question:\"\"\"\n",
    "            \n",
    "            response = await self.llm.ainvoke([HumanMessage(content=rewrite_prompt)])\n",
    "            rewritten_query = response.content.strip()\n",
    "            \n",
    "            print(f\"Original: {current_query}\")\n",
    "            print(f\"Rewritten: {rewritten_query}\")\n",
    "            \n",
    "            \n",
    "            # Store rewritten query for retrieval\n",
    "            state[\"rewritten_query\"] = rewritten_query\n",
    "            print(\"Debugging(in state) Rewritten query for follow-up:\", state.get(\"rewritten_query\"))\n",
    "        else:\n",
    "            state[\"rewritten_query\"] = current_query\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7c3ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # We can add Metadata filtering Here\n",
    "    async def retriever(self,state: AgentState):     \n",
    "        vectorstore = PGVector(\n",
    "            connection=CONNECTION_STRING,\n",
    "            collection_name=state[\"collection_name\"],\n",
    "            embeddings=self.embedding_model,\n",
    "            use_jsonb=True,\n",
    "            engine_args={\"poolclass\": NullPool}  # disable pooling\n",
    "        )\n",
    "        # Metadata filter for this specific PDF\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 5,\n",
    "                \"filter\": {\"doc_id\": state[\"doc_id\"],\"user_id\":state[\"user_id\"] } # only search this pdf  and for this user\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Use rewritten query instead of original\n",
    "        query = state.get(\"rewritten_query\", state[\"messages\"][-1].content)\n",
    "        \n",
    "        retrieved_docs = await run_in_threadpool(\n",
    "            retriever.invoke,query\n",
    "            )\n",
    "\n",
    "        state[\"retrieved_docs\"] = retrieved_docs\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d0777",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# file_name is added during text splitting\n",
    "    # page exists → PyPDFLoader adds this automatically\n",
    "    # as it is in the middile of our graph which is async we have to make it async also\n",
    "    async def context_builder(self,state:AgentState):\n",
    "            retrieved_docs = state.get(\"retrieved_docs\",[])\n",
    "            if not state[\"retrieved_docs\"]:\n",
    "                state[\"context\"] = \"\"\n",
    "                state[\"answer\"] = (\"I could not find relevant information in the provided document.\")\n",
    "            else:\n",
    "\n",
    "                context = \"\\n\\n\".join(\n",
    "                    f\"[Source: {doc.metadata.get('file_name', 'Unknown')} \"\n",
    "                    f\"- Page {doc.metadata.get('page', 'N/A')}]\\n\"    # page no\n",
    "                    f\"{doc.page_content}\"\n",
    "                    for doc in retrieved_docs\n",
    "                )\n",
    "                state[\"context\"] = context\n",
    "            \n",
    "            # Yield control back to event loop to avoid blocking\n",
    "            await asyncio.sleep(0)\n",
    "            return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7319e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def summary_creation(self,state:AgentState):\n",
    "        existing_summary = state[\"summary\"] # we first load existing summary\n",
    "\n",
    "        # We have two scenrio:\n",
    "        # 1. We might already have summary\n",
    "        # 2. or We are Genrating summary fir the first time\n",
    "        if existing_summary:\n",
    "            prompt = (\n",
    "                f\"Existing summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"Extend the summary using new conversation above\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = \"summarize the conversation above\"\n",
    "\n",
    "        message_for_summary = state[\"messages\"] + [HumanMessage(content=prompt)]\n",
    "\n",
    "        print(\"Callin summary LLM\") # debugging\n",
    "        # generate summary\n",
    "        print(\"Calling summary LLM\")\n",
    "        print(f\"DEBUG: token_usage BEFORE summary: {state.get('token_usage')}\")\n",
    "         \n",
    "        response = await self.llm.ainvoke(message_for_summary)\n",
    "\n",
    "        # now delete the orignal messages that have been summarized\n",
    "        message_to_delete = state[\"messages\"][:-2] if len(state[\"messages\"]) > 2 else []\n",
    "\n",
    "        return {\n",
    "            \"summary\":response.content,\n",
    "            \"messages\":[RemoveMessage(id=m.id) for m in message_to_delete]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7252f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def should_summzarizer(self,state:AgentState):\n",
    "        return len(state[\"messages\"]) > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4d21e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# cat node with memory\n",
    "    async def agent_response(self,state: AgentState):\n",
    "        \"\"\"\n",
    "        Generates the LLM response for the current query, injecting memory (summary or previous messages)\n",
    "        and RAG context into the prompt.\n",
    "        \"\"\"\n",
    "        context = state.get(\"context\", \"\")\n",
    "\n",
    "        # Get all human messages\n",
    "        human_messages = [m for m in state.get(\"messages\", []) if isinstance(m, HumanMessage)]\n",
    "        if not human_messages:\n",
    "            raise ValueError(\"No human message found in state for retrieval\")\n",
    "\n",
    "        query = human_messages[-1].content\n",
    "\n",
    "        prompt_messages = []\n",
    "\n",
    "        # Memory injection \n",
    "        # Use summary if it exists, otherwise include all previous messages\n",
    "        memory_text = state.get(\"summary\", \"\")\n",
    "        if not memory_text:\n",
    "            conversation_history = []\n",
    "            for m in state.get(\"messages\", []):\n",
    "                role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
    "                conversation_history.append(f\"{role}: {m.content}\")\n",
    "            memory_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n",
    "\n",
    "        # Inject memory as system message\n",
    "        prompt_messages.append(SystemMessage(content=f\"Conversation Memory:\\n{memory_text}\"))\n",
    "\n",
    "        # RAG context + current query \n",
    "        formatted_prompt = PROMPT_TEMPLATE.format(\n",
    "            context=context,\n",
    "            question=query\n",
    "        )\n",
    "        prompt_messages.append(HumanMessage(content=formatted_prompt))\n",
    "\n",
    "        print(\"Calling Agent Response LLM\")  # debugging\n",
    "\n",
    "        # response = self.llm.invoke(prompt_messages)\n",
    "        with get_openai_callback() as cb:\n",
    "            response = await self.llm.ainvoke(prompt_messages)\n",
    "            print(\"Total tokens:\", cb.total_tokens)   #Total tokens = question + answer (plus some extras)\n",
    "            print(\"Prompt tokens:\", cb.prompt_tokens)  # user query + system prompt + conversation history ==> everything before the model starts answering\n",
    "            print(\"Completion tokens:\", cb.completion_tokens)  # anser token generated by model(AI response)\n",
    "\n",
    "        # THIS WORK WHEN WE USE STREAMING\n",
    "        total_tokens = cb.total_tokens\n",
    "        prompt_tokens = cb.prompt_tokens\n",
    "        completion_tokens = cb.completion_tokens\n",
    "\n",
    "        #THIS WORK WITH OUT STREAMING BUT NOT WITH STREAMING\n",
    "        # token_usage = response.response_metadata.get(\"token_usage\", {})\n",
    "        # total_tokens = token_usage.get(\"total_tokens\", 0)\n",
    "        # print(f\"Response metadata: {response.response_metadata}\")\n",
    "        # print(f\"Total tokens: {total_tokens}\")\n",
    "        \n",
    "        # print(\"pushing tokens usage to supabase\")\n",
    "        # await run_in_threadpool(\n",
    "        #     lambda: supabase_client.table(\"usage\").insert({\n",
    "        #             \"user_id\": state[\"user_id\"],\n",
    "        #             \"doc_id\": state[\"doc_id\"],\n",
    "        #             \"total_tokens\": total_tokens,\n",
    "        #             \"prompt_tokens\":prompt_tokens,\n",
    "        #             \"completion_tokens\":completion_tokens,\n",
    "        #             \"query\": state[\"messages\"][-1].content,\n",
    "        #             \"answer\": response.content\n",
    "        #             }).execute()\n",
    "        #     )\n",
    "\n",
    "        # store token in state \n",
    "        state[\"token_usage\"] = {\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"query\": query,\n",
    "            \"answer\": response.content\n",
    "            }\n",
    "        # Save AI response in state\n",
    "        state[\"messages\"].append(AIMessage(content=response.content))\n",
    "        state[\"answer\"] = response.content\n",
    "\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e8211",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def conditional(self, state: AgentState):\n",
    "        if state.get(\"vectorstore_uploaded\", False):\n",
    "            return \"query_rewriter\"   # already exists → query\n",
    "        else:\n",
    "            return \"document_ingestion\"  # new → ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb33143",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# import psycopg\n",
    "\n",
    "# conn = psycopg.connect(\n",
    "#     CONNECTION_STRING,\n",
    "#     autocommit=True\n",
    "# )\n",
    "# # PostgresSaver --> LangGraph will automatically create its own tables in your Supabase Postgres database the first time it runs.\n",
    "# checkpointer = PostgresSaver(conn) \n",
    "# # It creates the internal tables in Supabase:\n",
    "# # checkpoints\n",
    "# # checkpoint_writes\n",
    "# # These tables store:\n",
    "# # AgentState snapshots\n",
    "# # Messages\n",
    "# # Node execution progress\n",
    "# # Thread state\n",
    "# checkpointer.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdb98a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "    # nodes\n",
    "workflow.add_node(\"document_ingestion\",document_ingestion)\n",
    "workflow.add_node(\"query_rewriter\", query_rewriter)\n",
    "workflow.add_node(\"retriever\", retriever)\n",
    "workflow.add_node(\"context_builder\", context_builder)\n",
    "workflow.add_node(\"agent_response\", agent_response)\n",
    "workflow.add_node(\"summarize\",summary_creation)\n",
    "workflow.add_node(\"check_pdf\",check_pdf_already_uploaded)\n",
    "workflow.add_node(\"set_doc_id\",set_doc_id)\n",
    "\n",
    "# edges\n",
    "workflow.add_edge(START, \"set_doc_id\")\n",
    "workflow.add_edge(\"set_doc_id\", \"check_pdf\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_pdf\",\n",
    "    nodes.conditional,\n",
    "    {\n",
    "        \"document_ingestion\": \"document_ingestion\",\n",
    "        \"query_rewriter\": \"query_rewriter\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# if new vector store path\n",
    "workflow.add_edge(\"document_ingestion\",\"query_rewriter\")\n",
    "\n",
    "workflow.add_edge(\"query_rewriter\", \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"context_builder\")\n",
    "workflow.add_edge(\"context_builder\", \"agent_response\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_response\",\n",
    "    nodes.should_summzarizer,\n",
    "    {\n",
    "        True: \"summarize\",\n",
    "        False: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=self.checkpointer)\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d67087",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project root\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "file_path = os.path.join(PROJECT_ROOT,\"data\",\"Constitution and law\",\"PAKISTAN PENAL CODE.pdf\")\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\"File is not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670769d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate unique doc_id for PDF\n",
    "doc_id = get_file_hash(file_path)\n",
    "\n",
    "# Create a collection name based on file name\n",
    "collection_name = (os.path.splitext(os.path.basename(file_path))[0].lower().replace(\" \", \"_\"))\n",
    "\n",
    "# Thread ID for chat persistence\n",
    "thread_id = \"user-123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7775ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Supabase Postgres for checkpointing\n",
    "with PostgresSaver.from_conn_string(CONNECTION_STRING) as checkpointer:\n",
    "    checkpointer.setup()  # run once\n",
    "\n",
    "    # Build the workflow graph\n",
    "    graph = GraphBuilder(checkpointer=checkpointer)()  # use as function\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # ===== FIRST MESSAGE =====\n",
    "    result = graph.invoke(\n",
    "        {\n",
    "            \"documents_path\": file_path,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"collection_name\": collection_name,\n",
    "            \"messages\": [HumanMessage(content=\"What is punishment for making false claim in court?\")],\n",
    "            \"summary\": \"\"},\n",
    "        config=config\n",
    "    ) \n",
    "    print(\"Answer 1:\", result[\"answer\"])\n",
    "\n",
    "\n",
    "    # ===== SECOND MESSAGE =====\n",
    "\n",
    "    initial_state = {\n",
    "    \"doc_id\": doc_id,\n",
    "    \"collection_name\": collection_name,  # Use existing vectorstore\n",
    "    \"messages\": [HumanMessage(content=\"What is the penalty for that?\")],  # Follow-up question\n",
    "}\n",
    "    result = graph.invoke(initial_state,config=config)\n",
    "    print(\"Answer 2:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab40aa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
